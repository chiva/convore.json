[{"user_id": 5565, "stars": [], "topic_id": 33610, "date_created": 1304504806.6445439, "message": "i'm tasked to develop some reporting (visitors, revenue, etc) tooling. i decided to go with ruby while writing all command line utilities using thor + datamapper (and putting a frontend built with rails on top of all this). opinions, suggestions, oppositions?", "group_id": 109, "id": 911531}, {"user_id": 22997, "stars": [], "topic_id": 33610, "date_created": 1304539668.833262, "message": "What kind of data are you going to store, and what is the expected data size?", "group_id": 109, "id": 919163}, {"user_id": 13404, "stars": [], "topic_id": 33610, "date_created": 1304609354.3270659, "message": "Did you already have a look at ruport? http://www.rubyreports.org/", "group_id": 109, "id": 930605}, {"user_id": 13097, "stars": [], "topic_id": 33610, "date_created": 1305507625.4003601, "message": "I used this Rails plugin to convert HTML files to PDF https://github.com/dagi3d/acts_as_flying_saucer", "group_id": 109, "id": 1064867}, {"user_id": 32812, "stars": [{"date_created": 1306327443.711998, "user_id": 12216}], "topic_id": 33610, "date_created": 1306222435.4448881, "message": "I've been working with Arel directly to generate reports. can be hard at first. still missing around though and getting the hang of it to figure out how to go abouts simplifying it to make a generalized reporting gem. most of the stuff I do just outputs csv so the marketing/sales/management people can work their excel pivot table magic on it", "group_id": 109, "id": 1163027}, {"user_id": 26653, "stars": [], "topic_id": 33610, "date_created": 1306306502.100559, "message": "The two other links that aren't ruport I mentioend are : https://github.com/wayneeseguin/dynamic_reports  and  https://github.com/acatighera/statistics", "group_id": 109, "id": 1175220}, {"user_id": 26653, "stars": [], "topic_id": 33610, "date_created": 1306306443.777144, "message": "@context Can you post some example in some pasties or on github? I know I'm looking at rurports and dynamic_reports and statistics on github right now and not sure what will be useful (and they all seem to have not been updated for a while).", "group_id": 109, "id": 1175218}, {"user_id": 32812, "stars": [], "topic_id": 33610, "date_created": 1306346342.0875931, "message": "@daryl here is a really basic example of arel usage, and two examples of how im currently using arel for a couple reports (messy i know. these are only gonna be used for a few months.) i would like to eventually come up with a nice clean way to put this into a gem to be more easily used.  If anyone has any sort of ideas that'd be awesome.  real basic arel usage: https://gist.github.com/919684 .  advanced arel usage in two reports: https://gist.github.com/991479", "group_id": 109, "id": 1181633}, {"user_id": 5565, "stars": [], "topic_id": 33610, "date_created": 1306345644.9480059, "message": "thx for the answers so far, ppl. appreciate it!", "group_id": 109, "id": 1181458}, {"user_id": 5565, "stars": [], "topic_id": 33610, "date_created": 1306345527.9830699, "message": "@peterhellberg we're running one big real estate search engine in germany. things like click targets for different timespans, how many clicks and ad revenue are key. there's also a white label product we're licensing which yields nearly the same data but limited to certain clients/areas/countries. the results get mailed to management. plain text tables with data are sufficient. my main concern is the best practice for the data mining part of all this. i.e. to what degree should i precalculate/accumulate numbers - depends on the degree of granulation that's being requested, right? or is it better to keep as much detailed data?", "group_id": 109, "id": 1181441}, {"user_id": 32812, "stars": [], "topic_id": 33610, "date_created": 1306346594.818985, "message": "@InSilenceStill depending on what you are doing maybe look into graphite and statsd. here is a good article to read: http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/ .  they explain (and kind of makes sense) is to lose the granularity after so long.  such as: stay very granular for the last 5 days, per 5/10 minute for the last two weeks. per hour the last 6 months, etc etc.  It kind of makes sense that the granularity is less important as the data gets older.  depending on the types of reports statsd/graphite might be exactly what you are looking for.", "group_id": 109, "id": 1181671}]